<b>Parallel Programming: (Graduate Program)</b>

Testing System: On Quanta cluster 11 nodes, each one has 12 processes, 96GB memory with 4TB storage space.
Each project must measure and analyze the performance and scalability of the implemented program.

<b>HW_1. Basic odd-even sort and advanced odd-even sort :</b> implementing basic and advanced odd-even sort with MPI.

<b>HW_2. Single Roller Coaster Car Problem And N-body Problem</b></p>
      <blockquote><blockquote><blockquote><b>Single Roller Coaster Car Problem  :</b></blockquote></blockquote></blockquote></p>
  <blockquote><blockquote><blockquote>2.1 Simulate Single Roller Coaster Car action and prevent synchronization by using conditional variable or mutex lock.</p></blockquote></blockquote></blockquote>
  <blockquote><blockquote><blockquote><b>N-body Problem:</b></blockquote></blockquote></blockquote></p>
  <blockquote><blockquote><blockquote>2.2 Parallel N-body's sequential code by using Pthread and OpenMP.</blockquote></blockquote></blockquote></p>
  <blockquote><blockquote><blockquote>2.3 Implement Barnes-Hut Algorithm by Pthread: Must parallelizing building tree phase and simulation phase</blockquote></blockquote></blockquote></p>

<b>HW_3. Manderlbrot Set Problem:</b> Implementing both the static and dynamic scheduling versions to evaluate its loading balance status.</p>
   <blockquote><blockquote><blockquote>3.1. distributed memory in MPI</blockquote></blockquote></blockquote></p> 
   <blockquote><blockquote><blockquote>3.2. shared memory in OpenMP</blockquote></blockquote></blockquote></p>
   <blockquote><blockquote><blockquote>3.3. hybrid (distributed-shared) memory ( MPI + OpenMP)</blockquote></blockquote></blockquote></p>

<b>HW_4. Blocked All-Pairs Shortest Paths Algorithm in CUDA:</b></p>
   <blockquote><blockquote><blockquote>4.1. Single GPU:implement Blocked All-Pairs Shortest Paths Algorithm in CUDA.</blockquote></blockquote></blockquote></p>
   <blockquote><blockquote><blockquote>4.2. Multi-GPU in MPI version:implement program using multiple GPUs on multi nodes by MPI.</blockquote></blockquote></blockquote></p>
   <blockquote><blockquote><blockquote>4.3. Multi-GPU in OpenMP version:mplement program using multiple GPUs on multi nodes by OpenMP.</blockquote></blockquote></blockquote></p>
